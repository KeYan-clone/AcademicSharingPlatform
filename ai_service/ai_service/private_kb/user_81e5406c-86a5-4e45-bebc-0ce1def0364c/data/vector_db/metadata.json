{
  "contents": [
    "Deep Learning Strategies for Effective Image\nForgery Detection and Localization\n1nd Nandini Kashyap 2rd Prince Yadav 3th Nikita\nComputer Science Engineering Computer Science Engineering Computer Science Engineering\nDelhi Technological University Delhi Technological University Delhi Technological University\nDelhi, India Delhi, India Delhi, India\nnandinikashyap101@gmail.com 2001prince03yadav@gmail.com nikitameena769@gmail.com\n4st Ms. Anukriti Kaushal\nComputer Science Engineering\nDelhi Technological University\nDelhi, India\nanukritikaushal30@gmail.com\nAbstract—The field of digital forensics assumes a crucial role information, rumors, or providing false testimony, thereby\nintheinvestigationofdigitalcrimes,withimageforgeryemerging posing significant societal risks. The emerging field of image\nas a critical area of concern. Delving into the intricacies of\nforensics, with its focus on distinguishing manipulated pho-\nimage forgery, the paper specifically addresses the challenges\ntographs,hasgarneredconsiderableattentioninbothscientific\nposed by splicing and copy/move techniques, underscoring the\nnecessity for advanced methods in accurate localization and and industrial realms. Its objective is to thwart forgers from\nsegmentation. Despite the existence of various techniques, per- employing such manipulated images for unethical purposes,\nsistent gaps remain in achieving precise forgery localization be it in business or politics.\nand segmentation, highlighting the imperative for robust models In recent times, the identification of altered regions relies\ncapable of handling diverse forgery scenarios. In this context,\non various forensic clues, such as copy/pasting, removal, and\nwe propose two distinctive models where VGG19 serves as\nthe backbone for U-Net, and ResNet acts as the backbone for splicing.Splicinghappenswhensegmentsofauthenticimages\nLinkNet.Leveragingtheinherentstrengthsofbothconvolutional are replicated and incorporated into different images, whereas\narchitectures, the depth of VGG19 seamlessly integrates with U- copy/pasting forgery entails duplicating and positioning por-\nNet,whileResNet’sresiduallearningcapabilitieseffectivelymeld tions within the confines of a single image. Despite thorough\nwithLinkNet,presentingaholisticapproachtoforgerydetection.\nexamination, pinpointing these manipulated areas remains a\nBothmodelsaredeployedtolocalizeandsegmentforgedregions\nwithin images. Comprehensive experimental evaluations confirm daunting challenge.\nthe efficiency of the suggested models when contrasted with Recently, conventional feature extraction-based approaches\nexisting techniques. Demonstrating superior performance, both in detecting forgery are classified into four primary groups:\nmodels achieve notable accuracy in forgery localization and\nthose centered on imaging device properties [1] [2] [3] [4],\nsegmentation tasks. The outcomes highlight the potential impact\nimage compression features [5] [6] [7], essence attributes [8]\nof these models in advancing the field of digital forensics,\nparticularlyinmitigatingchallengesassociatedwithsplicingand [9][10],andhashmethods[11][12][13].Whileclassifiedinto\ncopy/move attacks. distinct categories, these traditional approaches have inherent\nIndex Terms—Copy/Pasting attacks; VGG19; U-Net; ResNet; limitations. i) Techniques centered around image essence may\nLinkNet; Forgery Localization; fallshortifconcealedproceduresaccompanysplicingforgery.\nii)Detectionmethodsreliantonimagingdeviceattributesmay\nI. INTRODUCTION struggle when device noise levels are low. iii) Recognition\nLately, there has been a significant increase in both the based on image compression traits is restricted to identifying\navailability and advancement of user-friendly image editing imagesstoredinJPEGformat.iv)Hashingtechniquesdepend\nsoftware and techniques. This has considerably simplified on the hash of the unchanged original image, rendering\nand made image manipulation more affordable. The ease of reliance on a single forgery detection method unfeasible.\nmanipulating images raises concerns, as forgers can exploit Inresponsetothelimitationsoftraditionalforgerydetection\nthese tools to create deceptive photos, spreading misleading methods,thereisagrowingneedforadvancedtechniquesthat\ncan address the evolving landscape of image manipulation.\nWith the rapid progression of digital technologies, novel\nAuthorized licensed use limited to: BEIHANG UNIVERSITY. Downloaded on March 19,2025 at 03:14:20 UTC from IEEE Xplore. Restrictions apply.\n45272801.4202.661263E2CI/9011.01\n:IOD\n|\nEEEI\n4202©\n00.13$/42/4-3588-3053-8-979\n|\n)3E2CI(\nsnoitacilppA\nrieht\n&\ngnireenignE\nlacirtcelE\n,scinortcelE\n,retupmoC\nno\necnerefnoC\nlanoitanretnI\n4202",
    "approachesareessentialtoenhancetheaccuracyandreliability faced. ResNet34 is typically pre-trained on extensive datasets\nof forensic analyses. This paper delves into the exploration such as ImageNet, utilizing the broad knowledge acquired to\nof cutting-edge methodologies to overcome the drawbacks of improve its versatility across different localization or segmen-\ntraditional techniques and provide a more robust framework tation tasks.\nfor image forensics. In parallel, the proposed architecture integrates LinkNet\nThis research focuses on the specific challenges posed by because of its computational efficiency while maintaining\nsplicing and copy/pasting attacks, acknowledging their nu- segmentation accuracy. Operating on an encoder-decoder ar-\nanced nature and potential for evading conventional detection chitecture, LinkNet’s [14] encoder extracts pertinent features,\nmethods. The ability of forgers to exploit hidden processes while the decoder generates segmentation masks. Leveraging\nafter splicing forgery highlights the inadequacy of relying residual connections and skip connections, LinkNet adeptly\nsolely on image essence methods. Similarly, the vulnerability processes feature maps, preserving crucial spatial information\nof detection methods based on imaging device attributes to during the upsampling process.\nweaknoiseintensitynecessitatesashifttowardsmoreadaptive The fusion of ResNet34 with LinkNet is orchestrated\nand resilient approaches. through the amalgamation of their respective output feature\nAs we delve into the intricacies of image compression, it maps. These feature maps, enriched with comprehensive rep-\nbecomes evident that exclusive reliance on detecting JPEG resentations acquired by ResNet34, serve as the input for\nformatlimitationshamperstheversatilityofforgerydetection. the decoder component of LinkNet. This fusion capitalizes\nTherefore, a comprehensive methodology must encompass a on the high-quality features extracted by ResNet34, further\nbroaderspectrumofimageformatstoensuretheidentification refined and processed by LinkNet for precise segmentation\nof manipulated images, regardless of the compression method or localization. The integration of skip connections allows\nemployed. LinkNet’s decoder to access feature maps from earlier stages\nLately, hash techniques, despite their fundamental nature, of ResNet34, enabling the model to effectively utilize both\nencounter challenges when the original, unaltered image can- low-level and high-level features for segmentation.\nnot be accessed, thereby reducing their usefulness in real- During the model’s training phase, meticulous optimization\nworld forensic scenarios. Acknowledging these limitations, proceduresareemployed.Inputimages,standardizedtoapre-\nthereisanurgentneedtoinvestigatenovelmethodologiescon- determined shape, are fed into the model. Careful adjustment\nvolutional neural networks (CNNs) to improve the precision of the learning rate is pivotal to ensure stable convergence,\nand efficiency of forgery detection. typically favoring a lower learning rate to prevent overshoot-\nIn the subsequent sections of this paper, we present an ing during optimization. The optimization objective employs\nadvanced forgery detection framework that integrates state- binary cross-entropy loss to train the model in distinguishing\nof-the-art convolutional architectures, namely VGG19, U- between genuine and altered segments within images.\nNet, ResNet, and LinkNet. Leveraging the strengths of these Efficient management of training data, comprising images\narchitectures, our proposed models aim to provide a more and corresponding masks, is facilitated using TensorFlow’s\nencompassingsolutiontothechallengesposedbysplicingand Dataset API, enabling seamless batch processing and effi-\ncopy/pasting attacks. Through extensive experimental evalua- cient learning from extensive datasets. Furthermore, callbacks\ntions, we validate the efficacy of our models, demonstrating such as ModelCheckpoint and TensorBoard are integrated to\ntheirsuperiorperformancecomparedtotraditionaltechniques. monitor and enhance the training process. ModelCheckpoint\nThe findings of this research add to the ongoing discussion ensuresperiodicsavingofthebest-performingmodelweights,\nin image forensics and provide a promising direction for while TensorBoard provides visualizations and metrics for\naddressing the risks linked with image manipulation across comprehensive performance tracking.\ndifferent societal sectors.\nIn essence, the fusion of ResNet34 with LinkNet harnesses\nthestrengthsofbotharchitectures,resultinginasegmentation\nII. METHODOLOGY\nmodel capable of accurately localizing objects or regions of\nTo improve image forgery detection and localization, we\ninterest within images.\npropose two novel image localization models.\nA. Proposed Resnet34+Linknet model B. Proposed VGG19+UNet model\nn the proposed framework, ResNet34 serves as the founda- ThearchitecturalfusionofVGG19andUNetoffersanovel\ntional backbone model utilized for feature extraction, which approach to the task of image forgery detection, prioritizing\nis crucial for subsequent segmentation or localization tasks. intricate feature extraction and precise localization. VGG19,\nRenowned for its deep residual connections, ResNet34 ex- renowned for its depth and comprehensive convolutional lay-\ncels in capturing intricate details such as edges, textures, ers, serves as the foundational feature extractor in this hybrid\nand object components, essential for precise localization and model. Having been pre-trained on ImageNet, VGG19 inher-\nsegmentation. The inclusion of these residual connections is ently possesses a robust capability to capture nuanced details\nvital to ensure a consistent flow of gradients during training, withinimages,avitalaspectindiscerningsubtlemanipulations\neffectively mitigating the vanishing gradient issue commonly indicative of forgery.\nAuthorized licensed use limited to: BEIHANG UNIVERSITY. Downloaded on March 19,2025 at 03:14:20 UTC from IEEE Xplore. Restrictions apply.",
    "Simultaneously, the proposed architecture incorporates images are marked by black regions in the mask, with pixels\nUNet’s design [15], enhancing VGG19’s feature extraction set to 0.\ncapabilities. UNet’s encoder-decoder structure, along with The ResNet34 + LinkNet and VGG19 + U-Net models\nskip connections, allows the model to effectively handle the underwentthoroughevaluationusingadatasetcontainingma-\nextracted features. The encoder section accurately identifies nipulated images. The dataset encompassed diverse forms of\npertinentfeaturesfromtheinputimage,whilethedecoderpart, digitalimagemanipulation,suchascopy/pastingandsplicing.\nemploying upsampling techniques, reconstructs these features Model training employed an Adam optimizer with a fixed\ninto a comprehensive segmentation mask. The deliberate in- learning rate of 1e-4, and the training duration was capped\nclusion of skip connections enables the merging of high and at a maximum of 10 epochs.\nlow-level features, a crucial factor that enhances the precise\nB. Evaluation Metrices\nidentification of manipulated regions in the image.\nThe fusion of VGG19 with UNet takes place by combining 1.F1Score:Incorrectlydetectingmanipulatedpixels(FP),\ntheir individual output feature maps. The feature maps ex- preciselyidentifyingmanipulatedpixels(TP),anderroneously\ntracted by VGG19 are integrated with the decoder segment of identifying unaltered pixels (FN) are pivotal measures for\nUNet,whichisresponsibleforupsamplingandgeneratingthe evaluating the detection of image forgery localization. Recall,\nsegmentationmask.Thisfusionallowsthemodeltoeffectively Precision, and F1 Score are utilized as metrics for evaluation\nmerge deep contextual information from VGG19 with high- to gauge the effectiveness of the suggested algorithms in\nresolutionfeaturemapsfromUNet,leadingtoimprovedobject detecting manipulation in the conducted tests.\nlocalization and boundary delineation accuracy.\nMethodologically, the training protocol for the TP\nPrecision= (1)\nVGG19+UNet model mirrors that of its counterpart, the TP + FP\nResNet+LinkNet model. Input images are resized and\nTP\nsubsequently fed into the model for processing. Consistency Recall= (2)\nTP + FN\nis upheld in terms of the learning rate, loss function, and\noptimization algorithm, ensuring a standardized approach\nRecall·Precision\nF1 Score=2· (3)\nto training. The utilization of TensorFlow’s Dataset API Recall + Precision\nstreamlines the handling of training data, while the In this context, Recall, as described in Eq. (1), indicates\nintegration of callbacks such as ModelCheckpoint and the probability of accurately detecting manipulated areas in\nTensorBoard facilitates monitoring and optimization of the the ground truth image. Precision, formulated in Eq. (2),\ntraining process. represents the likelihood that the identified regions in the\nDespite the inherent computational demands attributed to groundtruthimagetrulycorrespondtomanipulatedareas.The\nVGG19’s depth and UNet’s expansive feature maps, the re- F1Score,articulatedinEq.(3),integratesRecallandPrecision\nsultant VGG19+UNet architecture emerges as a paradigm of into a unified metric to comprehensively evaluate the overall\nunparalleledperformance.Itsproficiencyincapturingintricate localization performance.\ndetails and accurately localizing image forgeries signifies a 2. Log Loss : Log Loss, commonly known as binary\nremarkable advancement in the domain of image forgery entropy loss, measures the accuracy of predicted probabilities\ndetection. compared to true labels. Log Loss Formula:\nN\nIII. EXPERIMENTALRESULTS 1 (cid:88)\nLog Loss=− [y log(p )+(1−y )log(1−p )]\nN i i i i\nA. Dataset i=1\nwhere N =number of samples,\nWe have considered the dataset from the IEEE IFS-TC\nImageForensicsChallenge[16],whichcomprisesacollection y i =true label of the i-th sample,\nofbothauthenticandmanipulatedimages.Thedatasetencom- p =predicted probability of the i-th sample.\ni\npassesboth’pristine’images,representingunalteredoriginals,\nC. Inferences\nand ’forged’ images, which have undergone modifications.\nTheseimagesoriginatefromdiversedigitalcamerascapturing Thefindingsderivedfromourresultssuggestahighdegree\nvarious indoor and outdoor scenes. Each forged image is of accuracy in identifying the forged regions’ boundaries.\naccompanied by a corresponding map image indicating the Thisisapparentfromtheperformanceobservedintheoutput.\nspecific regions that have been altered. In the provided data, the first column pertains to the forged\nThe dataset is segmented into distinct training sets, testing images, the central column displays the predicted mask of the\nsets, and validation sets, comprising a total of 1,273 test forged regions, and the last column shows the ground-truth\nimages, 3,893 train images, and 1,000 validation images. mask.\nWithinthetrainingset,pristineimagesaredenotedbyawhite\nmask,whereallpixelsaresetto255,indicatingtheabsenceof The assessment of our results on the IEEE IFS-TC Image\nanytampering.Conversely,regionsofmanipulationwithinthe Forensics Challenge dataset is presented in a comprehensive\nAuthorized licensed use limited to: BEIHANG UNIVERSITY. Downloaded on March 19,2025 at 03:14:20 UTC from IEEE Xplore. Restrictions apply.",
    "TABLEI real-worldscenarios.Ongoingresearchisessentialforcontinu-\nEVALUATIONOFPROPOSEDMODELLOCALIZATION ousrefinementandadaptationtoemergingchallengesinimage\nmanipulation.Ourproposedmodelscontributesignificantlyto\nModel EvaluationMetrics\nthe discourse on image forensics, paving the way for future\nArchitecture F1score logloss\ninnovations in forgery detection and localization.\nVGG19+Unet 0.8702 0.5801\nResNet+linknet 0.9557 0.3166\nTABLEII\nASSESSMENTOFLOCALIZATIONPERFORMANCEOFRECENTMODELSON\nCASIA\nModel F1Score\nRichFeature[18] 0.408\nLocalDescriptor[19] 0.58\nOurs(VGG19+Unet) 0.6579\nOurs(ResNet34+Linknet) 0.7147\ntable, showcasing the F1 score and log loss metrics for\nboth of our proposed models. Subsequent to this, we present\ncomparisons between our experimental results and several\nother techniques that have been previously employed on the\nCASIA dataset [17].\nFig.1. VisualComparison:InputImage,PredictedMask,andGroundTruth\nIn evaluating the performance of our proposed models for Mask\nimage forgery detection, we compared them against existing\ntechniques using two key metrics: F1 score and logloss. The REFERENCES\nresults, presented in Table I, demonstrate the effectiveness of\n[1] Y.-f. Hsu and S.-f. Chang, “Detecting image splicing using geometry\nour approach. Both models, employing VGG19 with U-Net invariantsandcameracharacteristicsconsistency,”pp.549–552,2006.\nandResNetwithLinkNetarchitectures,showedsignificantim- [2] M.K.JohnsonandH.Farid,“Exposingdigitalforgeriesthroughspecular\nhighlightsontheeye,”pp.311–325,2007.\nprovements over previous methods. Specifically, the ResNet-\n[3] B. Mahdian and S. Saic, Detection of Resampling Supplemented with\nbased model achieved an impressive F1 score of 0.9557 and a NoiseInconsistenciesAnalysisforImageForensics,2008.\nlogloss of 0.3166, surpassing the performance of the VGG19- [4] H.Gou,A.Swaminathan,andM.Wu,“Noisefeaturesforimagetamper-\ningdetectionandsteganalysis,”in2007IEEEInternationalConference\nbased model, which still produced commendable results with\nonImageProcessing,2007,vol.6,pp.VI–97–VI–100.\nan F1 score of 0.8702 and a logloss of 0.5801. [5] M. K. Johnson and H. Farid, Exposing Digital Forgeries in Complex\nFurthermore, when comparing our models’ performance on LightingEnvironments,2007,vol.2,no.3.\n[6] Z. Lin, J. He, X. Tang, and C.-K. Tang, “Fast, automatic and\nthe CASIA dataset against recent approaches, as detailed in\nfine-grained tampered jpeg image detection via dct coefficient\nTableII,itisevidentthatourResNet-basedmodeloutperforms analysis,” vol. 42, no. 11, 2009, pp. 2492–2501. [Online]. Available:\nexisting methods with an F1 score of 0.7147. The VGG19- https://www.sciencedirect.com/science/article/pii/S0031320309001198\n[7] S.Ye,Q.Sun,andE.-C.Chang,“Detectingdigitalimageforgeriesby\nbased model also demonstrates competitive performance with\nmeasuringinconsistenciesofblockingartifact,”pp.12–15,2007.\nan F1 score of 0.6579. These findings underscore the efficacy [8] W.Chen,Y.Shi,andW.Su,“Imagesplicingdetectionusing2-dphase\nof our proposed models in accurately localizing and segment- congruencyandstatisticalmomentsofcharacteristicfunction-art.no.\n65050r,”022007.\ning forged regions within images, thereby advancing the field\n[9] W.Wang,J.Dong,andT.Tan,“Effectiveimagesplicingdetectionbased\nof digital forensics and addressing challenges associated with onimagechroma,”pp.1257–1260,2009.\nimage forgery, particularly splicing and copy/move attacks. [10] X.Zhao,J.Li,S.Li,andS.Wang,“Detectingdigitalimagesplicingin\nchromaspaces,”pp.12–22,2011.\n[11] Z. Tang, X. Zhang, X. Li, and S. Zhang, “Robust image hashing with\nD. Conclusion\nringpartitionandinvariantvectordistance,”pp.200–214,2016.\n[12] X. Wang, K. Pang, X. Zhou, Y. Zhou, L. Li, and J. Xue, “A visual\nIn summary, our study offers compelling evidence of the\nmodel-based perceptual image hash for content authentication,” IEEE\neffectiveness of the proposed ResNet + LinkNet and VGG19 TransactionsonInformationForensicsandSecurity,vol.10,no.7,pp.\n+ UNet models for detecting and localizing image forgery. 1336–1349,2015.\n[13] C.-P.Yan,C.-M.Pun,andX.-C.Yuan,“Quaternion-basedimagehashing\nThe thorough evaluation conducted on the IEEE IFS-TC\nforadaptivetamperinglocalization,”IEEETransactionsonInformation\nImage Forensics Challenge dataset illustrates their superior ForensicsandSecurity,vol.11,no.12,pp.2664–2677,2016.\nperformance, delivering precise and dependable results. The [14] A.ChaurasiaandE.Culurciello,“Linknet:Exploitingencoderrepresen-\ntationsforefficientsemanticsegmentation,”pp.1–4,2017.\nvisualizations, metrics, and comparative analyses presented in\n[15] M. M. Qureshi and M. G. Qureshi, “Image forgery detection & local-\nthisstudyhighlightthepotentialapplicationsofourmodelsin izationusingregularizedu-net,”pp.434–442,2021.\nAuthorized licensed use limited to: BEIHANG UNIVERSITY. Downloaded on March 19,2025 at 03:14:20 UTC from IEEE Xplore. Restrictions apply.",
    "[16] “IEEE IFS-TC Image Forensics Challenge dataset.” [Online].\nAvailable:https://web.archive.org/web/20171013200331/http://ifc.recod.\nic.unicamp.br/fc.website/index.py?sec=5\n[17] “IEEEIFS-TCImageForensicsChallengedataset.”[Online].Available:\nhttps://paperswithcode.com/dataset/casia-v1\n[18] P.Zhou,X.Han,V.I.Morariu,andL.S.Davis,“Learningrichfeatures\nforimagemanipulationdetection,”pp.1053–1061,2018.\n[19] Y.Rao,J.Ni,andH.Zhao,“Deeplearninglocaldescriptorforimage\nsplicing detection and localization,” IEEE Access, vol. 8, pp. 25611–\n25625,2020.\nAuthorized licensed use limited to: BEIHANG UNIVERSITY. Downloaded on March 19,2025 at 03:14:20 UTC from IEEE Xplore. Restrictions apply."
  ],
  "sources": [
    "93e79b02-0b16-41b7-b01c-ca133506a209:Deep_Learning_Strategies_for_Effective_Image_Forgery_Detection_and_Localization.pdf 第1页",
    "93e79b02-0b16-41b7-b01c-ca133506a209:Deep_Learning_Strategies_for_Effective_Image_Forgery_Detection_and_Localization.pdf 第2页",
    "93e79b02-0b16-41b7-b01c-ca133506a209:Deep_Learning_Strategies_for_Effective_Image_Forgery_Detection_and_Localization.pdf 第3页",
    "93e79b02-0b16-41b7-b01c-ca133506a209:Deep_Learning_Strategies_for_Effective_Image_Forgery_Detection_and_Localization.pdf 第4页",
    "93e79b02-0b16-41b7-b01c-ca133506a209:Deep_Learning_Strategies_for_Effective_Image_Forgery_Detection_and_Localization.pdf 第5页"
  ]
}